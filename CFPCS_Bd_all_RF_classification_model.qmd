---
title: "Random forests to predict Bd infection status across all families of frogs"
author: Dani Wiley
date:   last-modified
editor: visual
description: |
  Part two which runs random forests on variables associated with sample taxonomy (i.e. family, species), demography (i.e. age class, sex), geography (i.e. latitude, longitude, elevation), and climate (i.e. temperature PCA, precipitation PCA). The spreadsheet, "All_Pathogen_Dataset_Metadata", is helpful for understanding the data. Contact dlfwiley[at]gmail.com with any questions or requests for more information.
 
format:
  html:
    theme:            litera
    highlight-style:  atom-one
    page-layout:      full      # article, full   # https://quarto.org/docs/output-formats/page-layout.html
    toc:              true
    toc-location:     body  # body, left, right
    number-sections:  false
    self-contained:         false     # !!! this can cause a render error
    code-overflow:          scroll    # scroll, wrap
    code-block-bg:          true
    code-block-border-left: "#30B0E0"
    code-copy:              false     # true, false, hover a copy buttom in top-right of code block
---

#Load packages 

Helpful links for understanding RF output :https://www.ibm.com/topics/random-forest

```{r}
library(dplyr)
library(randomForest)
library(ggraph)
library(igraph)
library(tidyverse)
library(caret)
library(caTools)
library(e1071)
library(ggplot2)
library(vip)
```

# Load data

```{r}
### LOAD FULL DATA
Bd_p_df <- read.csv("All_Pathogen_Dataset.csv")
#Bd prevalence dataset: contains all positive and negative individuals
Bd_p_df <- filter(Bd_p_df, !is.na(Bd_p_df$Status_Bd))
#Rename long variable (Elevation_30s,"Confirmed_Log10_Average_Bd[Pr,Rv]SQ")
names(Bd_p_df)[names(Bd_p_df) == "Fam"] <- "Family"
names(Bd_p_df)[names(Bd_p_df) == "Sp"] <- "Species"
names(Bd_p_df)[names(Bd_p_df) == "Elevation_30s"] <- "Elevation"
names(Bd_p_df)[names(Bd_p_df) == "Confirmed_Average_Bd_SQ"] <- "BdSQ"
names(Bd_p_df)[names(Bd_p_df) == "Confirmed_Average_Rv_SQ"] <- "RvSQ"
names(Bd_p_df)[names(Bd_p_df) == "Tissue_Type_Tested_Bd"] <- "Tissue"

#Making sure our variables are correctly classified.
Bd_p_df$Species   <- as.factor(Bd_p_df$Species)
Bd_p_df$Family    <- as.factor(Bd_p_df$Family)
Bd_p_df$Status_Bd <- as.factor(Bd_p_df$Status_Bd)
Bd_p_df$Age       <- as.factor(Bd_p_df$Age)
Bd_p_df$Sex       <- as.factor(Bd_p_df$Sex)
Bd_p_df$Month     <- as.factor(Bd_p_df$Month)
Bd_p_df$Year      <- as.factor(Bd_p_df$Year)

Bd_p_df$Tissue[Bd_p_df$Tissue=="Liver_included_no_skin"]  <-"Internal_tissues"
Bd_p_df$Tissue[Bd_p_df$Tissue=="Liver_muscle"]            <-"Internal_tissues"
Bd_p_df$Tissue[Bd_p_df$Tissue=="Liver_only"]              <-"Internal_tissues"
Bd_p_df$Tissue[Bd_p_df$Tissue=="Muscle_only"]             <-"Internal_tissues"
Bd_p_df$Tissue[Bd_p_df$Tissue=="Toe_only"]                <-"Includes_external"
Bd_p_df$Tissue[Bd_p_df$Tissue=="Whole_organism"]          <-"Includes_external"
Bd_p_df$Tissue[Bd_p_df$Tissue=="Liver_toe"]               <-"Includes_external"
Bd_p_df$Tissue[Bd_p_df$Tissue=="Muscle_toe"]              <-"Includes_external"

Bd_p_df.exto <- subset(Bd_p_df, 
                         Bd_p_df$Tissue == "Includes_external" )
Bd_p_df.inte <- subset(Bd_p_df, 
                         Bd_p_df$Tissue == "Internal_tissues" )

Bd_p_df$Tissue      <- as.factor(Bd_p_df$Tissue)
Bd_p_df.exto$Tissue <- as.factor(Bd_p_df.exto$Tissue) # 845
Bd_p_df.inte$Tissue <- as.factor(Bd_p_df.inte$Tissue) # 363

str(Bd_p_df)
```

#------------------------------------------------------
#STEP 1: Verify data
Check data for rows with missing values
```{r}
glimpse(Bd_p_df) #check data
#Checking NAs STEP 1
Bd_p_df %>%
  summarise(
    across(.cols = everything(),
           .fns = ~sum(is.na(.)))) %>%
  glimpse() 
```
#------------------------------------------------------
#STEP 2: Build datasets

Dropping variables with NAs and building model-specific datasets.
```{r}
dim(Bd_p_df)
RFfull_Bdp <- Bd_p_df %>% #FULL MODEL SET
  dplyr::select(Status_Bd
                , Species
                , Family
                , Sex
                , Age
                , Tissue
                , Latitude
                , Longitude
                , Elevation
                , TPC1
                , TPC2
                , PPC1
                , PPC2)
dim(RFfull_Bdp)

#Check to make sure there are no NAs
RFfull_Bdp %>%
  summarise(
    across(.cols = everything(),
           .fns = ~sum(is.na(.)))) %>%
  glimpse() 
#------------------------------------------------------------------------------------------
```

MODEL 1 
REDUCED MODEL SET (WITHOUT LONGITUDE)
```{r}
RF_Bdp.1 <- Bd_p_df %>% 
  dplyr::select(Status_Bd
                , Species
                , Family
                , Sex
                , Age
              #  , Tissue # Tissue is used to determine project design limitations - do not include in models 
                , Latitude
                , Elevation
                , TPC1
                , TPC2
                , PPC1
                , PPC2)
dim(RF_Bdp.1)

#External/combination tissues only:

RF_Bdp.1.exto <- Bd_p_df.exto %>% 
  dplyr::select(Status_Bd
                , Species
                , Family
                , Sex
                , Age
               # , Tissue
                , Latitude
                , Elevation
                , TPC1
                , TPC2
                , PPC1
                , PPC2)
dim(RF_Bdp.1.exto) 
```

MODEL 2 
REDUCED MODEL SET (WITHOUT LONGITUDE OR SPECIES)
```{r}
RF_Bdp.2 <- Bd_p_df %>% 
  dplyr::select(Status_Bd
                , Family
                , Sex
                , Age
               # , Tissue # Tissue is used to determine project design limitations - do not include in models 
                , Latitude
                , Elevation
                , TPC1
                , TPC2
                , PPC1
                , PPC2)
dim(RF_Bdp.2)
```

MODEL 3
REDUCED MODEL SET (WITHOUT SPECIES)
```{r}
RF_Bdp.3 <- Bd_p_df %>% 
  dplyr::select(Status_Bd
                , Family
                , Sex
                , Age
                , Tissue
                , Latitude
                , Longitude
                , Elevation
                , TPC1
                , TPC2
                , PPC1
                , PPC2)
dim(RF_Bdp.3)
```

#------------------------------------------------------
# STEP 3: Build original models
Run multiple Random Forest models and compare output

####RF Model 0: Full

```{r}
############################### RANDOM FORESTS ###############################
################# 0 - BD STATUS - FULL MODEL - 12 species #################
#############################################################################
#MODEL SET 0 (FULL)
set.seed(123)
rf0_statusBD <- randomForest(as.factor(Status_Bd)~.
                             , data=RFfull_Bdp
                             , ntree=5000
                             , nPerm=100
                             , proximity=TRUE
                             , importance=TRUE
                             )
print(rf0_statusBD) 
# THIS CALL BELOW WILL DIFFER AS RF OUTPUT IS SLIGHTLY DIFFERENT EVERY TIME.
# Call:
#  randomForest(formula = as.factor(Status_Bd) ~ ., data = RFfull_Bdp,      ntree = 5000, nPerm = 100, proximity = TRUE, importance = TRUE) 
# Type of random forest: classification
# Number of trees: 5000
# No. of variables tried at each split: 3
# 
# OOB estimate of  error rate: 12.65%
# Confusion matrix:
#      0   1 class.error
# 0 1007  58  0.05446009 # TERRIBLE AT PREDICTING INFECTED STATUS (Will fix in final model)
# 1  104 112  0.48148148

#append importance measures to dataframe
imp = importance(rf0_statusBD,scale=TRUE)
imp_temp <- data.frame(imp)
imp_temp$var <- row.names(imp_temp)
imp_temp
#In decreasing order, sort the variables by their average MeanDecreaseAccuracy
MDA <- imp_temp[order(imp_temp$MeanDecreaseAccuracy,decreasing=T),]
MDA
gini <- imp_temp[order(imp_temp$MeanDecreaseGini,decreasing=T),]
gini

#Plots with details:
RFfull_MDA.p  <- ggplot(MDA, 
  aes(x=reorder(var,-MeanDecreaseAccuracy),y=MeanDecreaseAccuracy)) +
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) + 
  theme(text = element_text(size=15), axis.text.x = element_text(angle=45,hjust=1)) + 
  ggtitle("Bd status - Full model") + 
  xlab("Variable") + 
  ylab("Importance (MeanDecAccuracy)")

RFfull_GINI.p <-ggplot(gini, aes(x=reorder(var,-MeanDecreaseGini),y=MeanDecreaseGini)) +
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) + 
  theme(text = element_text(size=15), axis.text.x = element_text(angle=45,hjust=1)) + 
  ggtitle("Bd status - Full model") + 
  xlab("Variable") + 
  ylab("Importance (MeanDecGini)")

varImpPlot(rf0_statusBD)

RFfull_MDA.p
RFfull_GINI.p
```

####RF Model 1 (-longitude)

```{r}
############################### RANDOM FORESTS ###############################
################# 1 - BD STATUS - NO LONGITUDE - 12 species #################
#############################################################################
#MODEL SET 1 (NO LONGITUDE)
set.seed(123)
rf1_statusBD <- randomForest(as.factor(Status_Bd)~.
                             , data=RF_Bdp.1, ntree=5000
                             , nPerm=100
                             , proximity=TRUE
                             , importance=TRUE
                             )
print(rf1_statusBD) 
# THIS CALL BELOW WILL DIFFER AS RF OUTPUT IS SLIGHTLY DIFFERENT EVERY TIME.
# Call:
#  randomForest(formula = as.factor(Status_Bd) ~ ., data = RF_Bdp.1,      ntree = 5000, nPerm = 100, proximity = TRUE, importance = TRUE) 
# Type of random forest: classification
# Number of trees: 5000
# No. of variables tried at each split: 3
# 
#         OOB estimate of  error rate: 13.04%
# Confusion matrix:
#      0   1 class.error
# 0 1008  57  0.05352113 # TERRIBLE AT PREDICTING INFECTED STATUS (Will fix in final model)
# 1  110 106  0.50925926

#append importance measures to dataframe
imp = importance(rf1_statusBD,scale=TRUE)
imp_temp <- data.frame(imp)
imp_temp$var <- row.names(imp_temp)
imp_temp
#In decreasing order, sort the variables by their average MeanDecreaseAccuracy
MDA <- imp_temp[order(imp_temp$MeanDecreaseAccuracy,decreasing=T),]
MDA
gini <- imp_temp[order(imp_temp$MeanDecreaseGini,decreasing=T),]
gini

#Plots with details:
RF1_MDA.p  <- ggplot(MDA, 
  aes(x=reorder(var,-MeanDecreaseAccuracy),y=MeanDecreaseAccuracy)) +
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) + 
  theme(text = element_text(size=15), axis.text.x = element_text(angle=45,hjust=1)) + 
  ggtitle("Bd status - Model set 1 - No longitude") + 
  xlab("Variable") + 
  ylab("Importance (MeanDecAccuracy)")

RF1_GINI.p <-ggplot(gini, aes(x=reorder(var,-MeanDecreaseGini),y=MeanDecreaseGini)) +
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) + 
  theme(text = element_text(size=15), axis.text.x = element_text(angle=45,hjust=1)) + 
  ggtitle("Bd status - Model set 1 - No longitude") + 
  xlab("Variable") + 
  ylab("Importance (MeanDecGini)")

varImpPlot(rf1_statusBD)
RF1_MDA.p
RF1_GINI.p
```

####RF Model 2 (-longitude, -species)

```{r}
############################### RANDOM FORESTS ###############################
############ 2 - BD STATUS - NO LONGITUDE or SPECIES - 12 species ###########
#############################################################################
#MODEL SET 2 (WITHOUT SPECIES OR LONGITUDE)
set.seed(123)
rf2_statusBD <- randomForest(as.factor(Status_Bd)~.
                             , data=RF_Bdp.2
                             , ntree=5000
                             , nPerm=100
                             , proximity=TRUE
                             , importance=TRUE
                             )
print(rf2_statusBD) 
# THIS CALL BELOW WILL DIFFER AS RF OUTPUT IS SLIGHTLY DIFFERENT EVERY TIME.
# Call:
#  randomForest(formula = as.factor(Status_Bd) ~ ., data = RF_Bdp.2,      ntree = 5000, nPerm = 100, proximity = TRUE, importance = TRUE) 
# Type of random forest: classification
# Number of trees: 5000
# No. of variables tried at each split: 3
# 
# OOB estimate of  error rate: 13.04%
# Confusion matrix:
#      0   1 class.error
# 0 1009  56  0.05258216 # TERRIBLE AT PREDICTING INFECTED STATUS (Will fix in final model)
# 1  111 105  0.51388889

#append importance measures to dataframe
imp = importance(rf2_statusBD,scale=TRUE)
imp_temp <- data.frame(imp)
imp_temp$var <- row.names(imp_temp)
imp_temp
#In decreasing order, sort the variables by their average MeanDecreaseAccuracy
MDA <- imp_temp[order(imp_temp$MeanDecreaseAccuracy,decreasing=T),]
MDA
gini <- imp_temp[order(imp_temp$MeanDecreaseGini,decreasing=T),]
gini

#Plots with details:
RF2_MDA.p  <- ggplot(MDA, 
  aes(x=reorder(var,-MeanDecreaseAccuracy),y=MeanDecreaseAccuracy)) +
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) + 
  theme(text = element_text(size=15), axis.text.x = element_text(angle=45,hjust=1)) + 
  ggtitle("Bd status - Model set 2 - No longitude or species") + 
  xlab("Variable") + 
  ylab("Importance (MeanDecAccuracy)")

RF2_GINI.p <-ggplot(gini, aes(x=reorder(var,-MeanDecreaseGini),y=MeanDecreaseGini)) +
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) + 
  theme(text = element_text(size=15), axis.text.x = element_text(angle=45,hjust=1)) + 
  ggtitle("Bd status - Model set 2 - No longitude or species") + 
  xlab("Variable") + 
  ylab("Importance (MeanDecGini)")

varImpPlot(rf2_statusBD)
RF2_MDA.p 
RF2_GINI.p
```

####RF Model 3 (-species)

```{r}
############################### RANDOM FORESTS ###############################
############ 3 - BD STATUS - NO SPECIES - 12 species ###########
#############################################################################
#MODEL SET 3 (WITHOUT SPECIES)
set.seed(123)
rf3_statusBD <- randomForest(as.factor(Status_Bd)~.
                             , data=RF_Bdp.3
                             , ntree=5000
                             , nPerm=100
                             , proximity=TRUE
                             , importance=TRUE
                             )
print(rf3_statusBD) 
# THIS CALL BELOW WILL DIFFER AS RF OUTPUT IS SLIGHTLY DIFFERENT EVERY TIME.
# Call:
#  randomForest(formula = as.factor(Status_Bd) ~ ., data = RF_Bdp.3,      ntree = 5000, nPerm = 100, proximity = TRUE, importance = TRUE) 
# Type of random forest: classification
# Number of trees: 5000
# No. of variables tried at each split: 3
# 
# OOB estimate of  error rate: 13.19%
# Confusion matrix:
#      0   1 class.error
# 0 1003  62  0.05821596 # TERRIBLE AT PREDICTING INFECTED STATUS (Will fix in final model)
# 1  107 109  0.49537037
imp = importance(rf3_statusBD,scale=TRUE)
imp_temp <- data.frame(imp)
imp_temp$var <- row.names(imp_temp)
imp_temp
#In decreasing order, sort the variables by their average MeanDecreaseAccuracy
MDA <- imp_temp[order(imp_temp$MeanDecreaseAccuracy,decreasing=T),]
MDA
gini <- imp_temp[order(imp_temp$MeanDecreaseGini,decreasing=T),]
gini

#Plots with details:
RF3_MDA.p  <- ggplot(MDA, 
  aes(x=reorder(var,-MeanDecreaseAccuracy),y=MeanDecreaseAccuracy)) +
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) + 
  theme(text = element_text(size=15), axis.text.x = element_text(angle=45,hjust=1)) + 
  ggtitle("Bd status - Model set 3 - No species") + 
  xlab("Variable") + 
  ylab("Importance (MeanDecAccuracy)")

RF3_GINI.p <-ggplot(gini, aes(x=reorder(var,-MeanDecreaseGini),y=MeanDecreaseGini)) +
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) + 
  theme(text = element_text(size=15), axis.text.x = element_text(angle=45,hjust=1)) + 
  ggtitle("Bd status - Model set 3 - No species") + 
  xlab("Variable") + 
  ylab("Importance (MeanDecGini)")

varImpPlot(rf3_statusBD)
RF3_MDA.p 
RF3_GINI.p
```

####RF Model 4 (-longitude, -external/combination tissues only)

```{r}
set.seed(123)
#WITHOUT LONGITUDE, WITHOUT SOUTHEAST REGION SPECIES (with high internal-only tissues)
rf1_statusBD.exto <- randomForest(as.factor(Status_Bd)~.
                             , data=RF_Bdp.1.exto
                             , ntree=5000
                             , nPerm=100
                             , proximity=TRUE
                             , importance=TRUE
                             #, classwt = c(6,1) <- this can be used to penalize minority
                             )
print(rf1_statusBD.exto) 
# THIS CALL BELOW WILL DIFFER AS RF OUTPUT IS SLIGHTLY DIFFERENT EVERY TIME.
# Call:
#  randomForest(formula = as.factor(Status_Bd) ~ ., data = RF_Bdp.1.exto,      ntree = 5000, nPerm = 100, proximity = TRUE, importance = TRUE) 
#                Type of random forest: classification
#                      Number of trees: 5000
# No. of variables tried at each split: 3
# 
#         OOB estimate of  error rate: 17.87%
# Confusion matrix:
#     0   1 class.error
# 0 589  58  0.08964451 # TERRIBLE AT PREDICTING INFECTED STATUS (Will fix in final model)
# 1  93 105  0.46969697

#append importance measures to dataframe
imp = importance(rf1_statusBD.exto,scale=TRUE)
imp_temp <- data.frame(imp)
imp_temp$var <- row.names(imp_temp)
imp_temp
#In decreasing order, sort the variables by their average MeanDecreaseAccuracy
MDA <- imp_temp[order(imp_temp$MeanDecreaseAccuracy,decreasing=T),]
MDA
gini <- imp_temp[order(imp_temp$MeanDecreaseGini,decreasing=T),]
gini

#Plots with details:
RF1_MDA.p.exto  <- ggplot(MDA, 
  aes(x=reorder(var,-MeanDecreaseAccuracy),y=MeanDecreaseAccuracy)) +
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) + 
  theme(text = element_text(size=15), axis.text.x = element_text(angle=45,hjust=1)) + 
  ggtitle("Bd status - Model set 1 - No longitude") + 
  xlab("Variable") + 
  ylab("Importance (MeanDecAccuracy)")

RF1_GINI.p.exto <-ggplot(gini, aes(x=reorder(var,-MeanDecreaseGini),y=MeanDecreaseGini)) +
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) + 
  theme(text = element_text(size=15), axis.text.x = element_text(angle=45,hjust=1)) + 
  ggtitle("Bd status - Model set 1 - No longitude") + 
  xlab("Variable") + 
  ylab("Importance (MeanDecGini)")

varImpPlot(rf1_statusBD.exto)
RF1_MDA.p.exto
RF1_GINI.p.exto
```

#------------------------------------------------------
# STEP 4: Compare model output
Picking best model via OOB & mtry
####RF Model 0: Full
```{r}
library(vip)
#FINDING MTRY FOR FULL MODEL
####3. Find the optimal mtry value#### - FULL MODEL
stBD_0 <- as.factor(RFfull_Bdp$Status_Bd)
class(stBD_0)
mtry_stBD_0 <- tuneRF(RFfull_Bdp[-1]
                      , stBD_0
                      , ntreeTry = 500
                      , stepFactor = 1.5
                      , improve = 0.01
                      , trace = T
                      , plot = T
                      )
best.m0 <- mtry_stBD_0[mtry_stBD_0[, 2] == min(mtry_stBD_0[, 2]), 1]
print(mtry_stBD_0)
print(best.m0) 
```

####RF Model 1 (-longitude)
```{r}
#FINDING MTRY FOR MODEL 1 (WITHOUT LONGITUDE)
stBD_1 <- as.factor(RF_Bdp.1$Status_Bd)
class(stBD_1)
mtry_stBD_1 <- tuneRF(RF_Bdp.1[-1]
                      , stBD_1
                      , ntreeTry = 500
                      , stepFactor = 1.5
                      , improve = 0.01
                      , trace = T
                      , plot = T
                      )
best.m1 <- mtry_stBD_1[mtry_stBD_1[, 2] == min(mtry_stBD_1[, 2]), 1]
print(mtry_stBD_1)
print(best.m1) 
```

####RF Model 2 (-longitude, -species)
```{r}
#FINDING MTRY FOR MODEL 2 (WITHOUT LONGITUDE OR SPECIES)
stBD_2 <- as.factor(RF_Bdp.2$Status_Bd)
class(stBD_2)
mtry_stBD_2 <- tuneRF(RF_Bdp.2[-1]
                      , stBD_2
                      , ntreeTry = 500
                      , stepFactor = 1.5
                      , improve = 0.01
                      , trace = T
                      , plot = T
                      )
best.m2 <- mtry_stBD_2[mtry_stBD_2[, 2] == min(mtry_stBD_2[, 2]), 1]
print(mtry_stBD_2)
print(best.m2) 
```

####RF Model 3 (-species)
```{r}
#FINDING MTRY FOR MODEL 3 (WITHOUT SPECIES)
stBD_3 <- as.factor(RF_Bdp.3$Status_Bd)
class(stBD_3)
mtry_stBD_3 <- tuneRF(RF_Bdp.3[-1]
                      , stBD_3
                      , ntreeTry = 500
                      , stepFactor = 1.5
                      , improve = 0.01
                      , trace = T
                      , plot = T
                      )
best.m3 <- mtry_stBD_3[mtry_stBD_3[, 2] == min(mtry_stBD_3[, 2]), 1]
print(mtry_stBD_3)
print(best.m3) 
#BEST MTRY FOR FULL MODEL: 2 MTRY; OOB ERROR AT ~13.5%
```

####RF Model 4 (-longitude, -external/combination tissues only)
```{r}
#FINDING MTRY FOR MODEL 4 (WITHOUT LONGITUDE and not considering southeastern species)
stBD_4 <- as.factor(RF_Bdp.1.exto$Status_Bd)
class(stBD_4)
mtry_stBD_4 <- tuneRF(RF_Bdp.1.exto[-1]
                      , stBD_4
                      , ntreeTry = 500
                      , stepFactor = 1.5
                      , improve = 0.01
                      , trace = T
                      , plot = T
                      )
best.m4 <- mtry_stBD_4[mtry_stBD_4[, 2] == min(mtry_stBD_4[, 2]), 1]
print(mtry_stBD_4)
print(best.m4) 
#BEST MTRY FOR MODEL 1: 2 MTRY; OOB ERROR AT  22.1%
```
#------------------------------------------------------
# STEP 5: Assess model accuracy

Training data set
#### RF Model 0: Full

```{r}
#No longitude
split_ds_Bdp <- sample(c(TRUE, FALSE), nrow(RFfull_Bdp), replace = TRUE, prob = c(0.7, 0.3))
train_ds_Bdp <- RFfull_Bdp[split_ds_Bdp, ]
test_ds_Bdp <- RFfull_Bdp[!split_ds_Bdp, ]
dim(train_ds_Bdp)
dim(test_ds_Bdp)

#RF######
rf_Bdp_full <- randomForest(as.factor(Status_Bd)~.
                        , data=train_ds_Bdp
                        , ntree=5000
                        , nPerm=100
                        , proximity=TRUE
                        , mtry = best.m0[1] # selecting the mtry from step 4
                        , importance=TRUE)
print(rf_Bdp_full) 
# THIS CALL BELOW WILL DIFFER AS RF OUTPUT IS SLIGHTLY DIFFERENT EVERY TIME.
# Call:
#  randomForest(formula = as.factor(Status_Bd) ~ ., data = train_ds_Bdp,      ntree = 5000, nPerm = 100, proximity = TRUE, mtry = best.m0[1],      importance = TRUE) 
#                Type of random forest: classification
#                      Number of trees: 5000
# No. of variables tried at each split: 2
# 
#         OOB estimate of  error rate: 12.08%
# Confusion matrix:
#     0  1 class.error
# 0 708 34   0.0458221
# 1  73 71   0.5069444

#Prediction & Confusion Matrix – train data
confusionMatrix(predict(rf_Bdp_full, train_ds_Bdp),train_ds_Bdp$Status_Bd, positive = '1')
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction   0   1
#          0 748  30
#          1  13 117
#                                           
#                Accuracy : 0.9526          
#                  95% CI : (0.9367, 0.9655)
#     No Information Rate : 0.8381          
#     P-Value [Acc > NIR] : < 2e-16         
#                                           
#                   Kappa : 0.8169          
#                                           
#  Mcnemar's Test P-Value : 0.01469         
#                                           
#             Sensitivity : 0.7959          
#             Specificity : 0.9829          
#          Pos Pred Value : 0.9000          
#          Neg Pred Value : 0.9614          
#              Prevalence : 0.1619          
#          Detection Rate : 0.1289          
#    Detection Prevalence : 0.1432          
#       Balanced Accuracy : 0.8894          
#                                           
#        'Positive' Class : 1  
```

####RF Model 1 (-longitude)
```{r}
split_ds_Bdp <- sample(c(TRUE, FALSE), nrow(RF_Bdp.1), replace = TRUE, prob = c(0.7, 0.3))
train_ds_Bdp <- RF_Bdp.1[split_ds_Bdp, ]
test_ds_Bdp <- RF_Bdp.1[!split_ds_Bdp, ]
dim(train_ds_Bdp)
dim(test_ds_Bdp)

#RF######
rf1_Bdp <- randomForest(as.factor(Status_Bd)~.
                        , data=train_ds_Bdp
                        , ntree=5000
                        , nPerm=100
                        , proximity=TRUE
                        , mtry = best.m1[1] # selecting the mtry from step 4
                        , importance=TRUE)
print(rf1_Bdp) 
# THIS CALL BELOW WILL DIFFER AS RF OUTPUT IS SLIGHTLY DIFFERENT EVERY TIME.
# Call:
#  randomForest(formula = as.factor(Status_Bd) ~ ., data = train_ds_Bdp,      ntree = 5000, nPerm = 100, proximity = TRUE, mtry = best.m1[1],      importance = TRUE) 
# Type of random forest: classification
# Number of trees: 5000
# No. of variables tried at each split: 2
# 
# OOB estimate of  error rate: 12.18%
# Confusion matrix:
#     0  1 class.error
# 0 723 37  0.04868421
# 1  73 70  0.51048951

varImpPlot(rf1_Bdp)

m1.1 <- vip(rf1_Bdp, geom = "point",
          mapping = NULL,
          aesthetics = list(),
          horizontal = TRUE,
          include_type = TRUE)
BdpRF1 <- m1.1 + geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) + ggtitle("Random Forest classification - Model 1 Bd infection status")
BdpRF1

importance(rf1_Bdp)

varImpPlot(rf1_Bdp)
m_bdp <- vip(rf1_Bdp, geom = "point",
          mapping = NULL,
          aesthetics = list(),
          horizontal = TRUE,
          include_type = TRUE)
m_bdp <- m_bdp + geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) + ggtitle("Random Forest classification - Trained Model Set for Bd infection status")
m_bdp

## Look at variable importance:
round(importance(rf1_Bdp), 2)

#Prediction & Confusion Matrix – train data
confusionMatrix(predict(rf1_Bdp, train_ds_Bdp),train_ds_Bdp$Status_Bd, positive = '1')
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction   0   1
#          0 754  15
#          1   6 128
#                                           
#                Accuracy : 0.9767          
#                  95% CI : (0.9647, 0.9855)
#     No Information Rate : 0.8416          
#     P-Value [Acc > NIR] : < 2e-16         
#                                           
#                   Kappa : 0.9105          
#                                           
#  Mcnemar's Test P-Value : 0.08086         
#                                           
#             Sensitivity : 0.8951          
#             Specificity : 0.9921          
#          Pos Pred Value : 0.9552          
#          Neg Pred Value : 0.9805          
#              Prevalence : 0.1584          
#          Detection Rate : 0.1417          
#    Detection Prevalence : 0.1484          
#       Balanced Accuracy : 0.9436          
#                                           
#        'Positive' Class : 1  
```

#### RF Model 2 (-longitude, -species)
```{r}
split_ds_Bdp <- sample(c(TRUE, FALSE), nrow(RF_Bdp.2), replace = TRUE, prob = c(0.7, 0.3))
train_ds_Bdp <- RF_Bdp.2[split_ds_Bdp, ]
test_ds_Bdp <- RF_Bdp.2[!split_ds_Bdp, ]
dim(train_ds_Bdp)
dim(test_ds_Bdp)

#RF######
rf2_Bdp <- randomForest(as.factor(Status_Bd)~.
                        , data=train_ds_Bdp
                        , ntree=5000
                        , nPerm=100
                        , proximity=TRUE
                        , mtry = best.m2[1] # selecting the mtry from step 4
                        , importance=TRUE)
print(rf2_Bdp) 
# THIS CALL BELOW WILL DIFFER AS RF OUTPUT IS SLIGHTLY DIFFERENT EVERY TIME.
# Call:
#  randomForest(formula = as.factor(Status_Bd) ~ ., data = train_ds_Bdp,      ntree = 5000, nPerm = 100, proximity = TRUE, mtry = best.m2[1],      importance = TRUE) 
#                Type of random forest: classification
#                      Number of trees: 5000
# No. of variables tried at each split: 2
# 
#         OOB estimate of  error rate: 12.33%
# Confusion matrix:
#     0  1 class.error
# 0 713 32  0.04295302
# 1  78 69  0.53061224

#Prediction & Confusion Matrix – train data
confusionMatrix(predict(rf2_Bdp, train_ds_Bdp),train_ds_Bdp$Status_Bd, positive = '1')
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction   0   1
#          0 740  17
#          1   5 130
#                                           
#                Accuracy : 0.9753          
#                  95% CI : (0.9629, 0.9845)
#     No Information Rate : 0.8352          
#     P-Value [Acc > NIR] : < 2e-16         
#                                           
#                   Kappa : 0.9074          
#                                           
#  Mcnemar's Test P-Value : 0.01902         
#                                           
#             Sensitivity : 0.8844          
#             Specificity : 0.9933          
#          Pos Pred Value : 0.9630          
#          Neg Pred Value : 0.9775          
#              Prevalence : 0.1648          
#          Detection Rate : 0.1457          
#    Detection Prevalence : 0.1513          
#       Balanced Accuracy : 0.9388          
#                                           
#        'Positive' Class : 1   
```

#### RF Model 3 (-species)

```{r}
split_ds_Bdp <- sample(c(TRUE, FALSE), nrow(RF_Bdp.3), replace = TRUE, prob = c(0.7, 0.3))
train_ds_Bdp <- RF_Bdp.3[split_ds_Bdp, ]
test_ds_Bdp <- RF_Bdp.3[!split_ds_Bdp, ]
dim(train_ds_Bdp)
dim(test_ds_Bdp)

#RF######
rf3_Bdp <- randomForest(as.factor(Status_Bd)~.
                        , data=train_ds_Bdp
                        , ntree=5000
                        , nPerm=100
                        , proximity=TRUE
                        , mtry = best.m3[1] # selecting the mtry from step 4
                        , importance=TRUE)
print(rf3_Bdp) 
# THIS CALL BELOW WILL DIFFER AS RF OUTPUT IS SLIGHTLY DIFFERENT EVERY TIME.
# Call:
#  randomForest(formula = as.factor(Status_Bd) ~ ., data = train_ds_Bdp,      ntree = 5000, nPerm = 100, proximity = TRUE, mtry = best.m3[1],      importance = TRUE) 
#                Type of random forest: classification
#                      Number of trees: 5000
# No. of variables tried at each split: 2
# 
#         OOB estimate of  error rate: 14.27%
# Confusion matrix:
#     0  1 class.error
# 0 735 42  0.05405405
# 1  90 58  0.60810811

#Prediction & Confusion Matrix – train data
confusionMatrix(predict(rf3_Bdp, train_ds_Bdp),train_ds_Bdp$Status_Bd, positive = '1')
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction   0   1
#          0 771  18
#          1   6 130
#                                           
#                Accuracy : 0.9741          
#                  95% CI : (0.9616, 0.9833)
#     No Information Rate : 0.84            
#     P-Value [Acc > NIR] : < 2e-16         
#                                           
#                   Kappa : 0.9002          
#                                           
#  Mcnemar's Test P-Value : 0.02474         
#                                           
#             Sensitivity : 0.8784          
#             Specificity : 0.9923          
#          Pos Pred Value : 0.9559          
#          Neg Pred Value : 0.9772          
#              Prevalence : 0.1600          
#          Detection Rate : 0.1405          
#    Detection Prevalence : 0.1470          
#       Balanced Accuracy : 0.9353          
#                                           
#        'Positive' Class : 1  
```

####RF Model 4 (-longitude, -external/combination tissues only)

```{r}
split_ds_Bdp <- sample(c(TRUE, FALSE), nrow(RF_Bdp.1.exto), replace = TRUE, prob = c(0.7, 0.3))
train_ds_Bdp <- RF_Bdp.1.exto[split_ds_Bdp, ]
test_ds_Bdp <- RF_Bdp.1.exto[!split_ds_Bdp, ]
dim(train_ds_Bdp)
dim(test_ds_Bdp)

#RF######
rf4_Bdp <- randomForest(as.factor(Status_Bd)~.
                        , data=train_ds_Bdp
                        , ntree=5000
                        , nPerm=100
                        , proximity=TRUE
                        , mtry = best.m4[1] # selecting the mtry from step 4
                        , importance=TRUE)
print(rf4_Bdp) 
# THIS CALL BELOW WILL DIFFER AS RF OUTPUT IS SLIGHTLY DIFFERENT EVERY TIME.
# Call:
#  randomForest(formula = as.factor(Status_Bd) ~ ., data = train_ds_Bdp,      ntree = 5000, nPerm = 100, proximity = TRUE, mtry = best.m4[1],      importance = TRUE) 
#                Type of random forest: classification
#                      Number of trees: 5000
# No. of variables tried at each split: 2
# 
#         OOB estimate of  error rate: 16.75%
# Confusion matrix:
#     0  1 class.error
# 0 436 34  0.07234043
# 1  66 61  0.51968504

#Prediction & Confusion Matrix – train data
confusionMatrix(predict(rf4_Bdp, train_ds_Bdp),train_ds_Bdp$Status_Bd, positive = '1')
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction   0   1
#          0 469  14
#          1   1 113
#                                           
#                Accuracy : 0.9749          
#                  95% CI : (0.9589, 0.9859)
#     No Information Rate : 0.7873          
#     P-Value [Acc > NIR] : < 2.2e-16       
#                                           
#                   Kappa : 0.9221          
#                                           
#  Mcnemar's Test P-Value : 0.001946        
#                                           
#             Sensitivity : 0.8898          
#             Specificity : 0.9979          
#          Pos Pred Value : 0.9912          
#          Neg Pred Value : 0.9710          
#              Prevalence : 0.2127          
#          Detection Rate : 0.1893          
#    Detection Prevalence : 0.1910          
#       Balanced Accuracy : 0.9438          
#                                           
#        'Positive' Class : 1  
```
We will select model 1 (-longitude) for our final model.
Now we can move forward with training and testing the model over 100 iterations and averaging the output.
#------------------------------------------------------
# STEP 6 Assess data structure
Examine imbalance and classification errors

```{r}
####################################################################
#### EXAMINE IMBALANCE AND CLASSIFICATION ERRORS

# Check imbalance in the dataset
ggplot(data = RF_Bdp.1) + geom_bar(mapping = aes(x = Status_Bd)) + ggtitle("Bd infection status distribution")

## See how many were misclassified.
#get prediction probability for each group
FamilyPredicted <- rf1_statusBD$votes
#get Family names and infection status from data
FamilyInfo <- RF_Bdp.1[c(1,3)]
FamilyPredicted <- cbind(FamilyPredicted,FamilyInfo)

#see what this information looks like
head(FamilyPredicted)

#output these predictions to a file
#write.csv(FamilyPredicted, file="RF_model_misclassified.csv", row.names=FALSE)

#visualize the predictions for each group.
#put predictions for each category in separate data frame
FamilyPredicted_Pos <- FamilyPredicted[ which(FamilyPredicted$Status_Bd == '1'), 1:2]
FamilyPredicted_Neg <- FamilyPredicted[ which(FamilyPredicted$Status_Bd == '0'), 1:2]
FamilyPredicted_Pos

#make plots
par(mfrow=c(1,2))
barplot(as.matrix(FamilyPredicted_Pos), col ="black", main = "Pos")
barplot(as.matrix(FamilyPredicted_Neg), col ="black", main = "Neg")
```

There is class imbalance which will need to be fixed - as it stands now, our model is very good at predicting majority class (uninfected), but very bad at predicting our class of interest (minority).
#------------------------------------------------------
# STEP 7: Balancing out models
####Upsampling
```{r}
#declare data frames to hold error rates and variable importance measures
RF_model_error_u = data.frame()
RF_varimp_u = data.frame()
#declare training datasets for both classes
Bd0=RF_Bdp.1[RF_Bdp.1$Status_Bd=="0",]
Bd1=RF_Bdp.1[RF_Bdp.1$Status_Bd=="1",]
#determine downsample.size by the minimum number of observations between the two class datasets
upsample.size <- max(c(nrow(Bd0), nrow(Bd1)))
upsample.size #1065

for (i in 1:100) {

  #sample from the larger dataset at the upsample size, or size of the smaller dataset
   Bd1samp <- Bd1[(sample(nrow(Bd1), size=(upsample.size), replace = TRUE)),] #UPSAMPLE SIZE
  #create training dataset to be used in random forest function
   pred_samp_u=rbind(Bd0, Bd1samp)
   #construct random forest object
  RF_model_u <- randomForest(as.factor(Status_Bd)~.
                   , data=pred_samp_u, ntree=2000, importance=TRUE
                  # , classwt = c(1,6)
                   , replace = TRUE,
                   , strata = as.factor(pred_samp_u[,"Status_Bd"])
                   , sampsize = c(500:500)
                   , mtry = best.m1[1]
                  )
  #extract the final estimate of OOB error rates
  err_u=RF_model_u$err.rate[nrow(RF_model_u$err.rate),]
  #append OOB error estimate to dataframe
  RF_model_error_u           <- rbind(RF_model_error_u, err_u)
  colnames(RF_model_error_u) <- names(err_u)
  #extract the importance measures
  imp_u = importance(RF_model_u,scale=TRUE)
  #append importance measures to dataframe
  imp_temp_u <- data.frame(imp_u)
  imp_temp_u$var <- row.names(imp_temp_u)
  RF_varimp_u <- rbind(RF_varimp_u, imp_temp_u)
}

up_Bd_CM <- confusionMatrix(predict(RF_model_u, pred_samp_u), pred_samp_u$Status_Bd, positive = '1')
up_Bd_CM
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction    0    1
#          0  945   24
#          1  120 1041
#                                           
#                Accuracy : 0.9324          
#                  95% CI : (0.9209, 0.9427)
#     No Information Rate : 0.5             
#     P-Value [Acc > NIR] : < 2.2e-16       
#                                           
#                   Kappa : 0.8648          
#                                           
#  Mcnemar's Test P-Value : 2.44e-15        
#                                           
#             Sensitivity : 0.9775          
#             Specificity : 0.8873          
#          Pos Pred Value : 0.8966          
#          Neg Pred Value : 0.9752          
#              Prevalence : 0.5000          
#          Detection Rate : 0.4887          
#    Detection Prevalence : 0.5451          
#       Balanced Accuracy : 0.9324          
#                                           
#        'Positive' Class : 1   

fourfoldplot(as.table(up_Bd_CM),color=c("red","#40B0A6"),main = "Confusion Matrix" )
```

Upsampling validation and assessment

```{r}
####################################################################
#### EXAMINE CLASSIFICATION ERRORS
## See how many were misclassified.
#get prediction probability for each group
Predicted_u <- RF_model_u$votes
#get Family names and infection status from data
SampleInfo_u <- pred_samp_u[c(1,3)]
StatusPredicted_u <- cbind(Predicted_u,SampleInfo_u)
#see what this information looks like
head(StatusPredicted_u)
#output these predictions to a file
#write.csv(StatusPredicted, file="RF_FINAL_model_misclassified.csv", row.names=FALSE)
#visualize the predictions for each group.
#put predictions for each category in separate data frame
StatusPredicted_Pos_u <- StatusPredicted_u[ which(StatusPredicted_u$Status_Bd == '1'), 1:2]
StatusPredicted_Neg_u <- StatusPredicted_u[ which(StatusPredicted_u$Status_Bd == '0'), 1:2]
StatusPredicted_Pos_u
#make plots
par(mfrow=c(1,2))
barplot(as.matrix(StatusPredicted_Pos_u), col ="black", main = "Pos")
barplot(as.matrix(StatusPredicted_Neg_u), col ="black", main = "Neg")
RF_model_error_u
colMeans(RF_model_error_u,na.rm=T)
#        OOB          0          1 
# 0.10189671 0.16782160 0.03597183 
#write.csv(RF_model_error_u,file = "RF_model_error_nolongitude_upsampling.csv",row.names=F)
```

```{r}
# Now that variable importance data is stored, extract the MDA for all variables from each random forest and summarize.
#isolate all MDA estimates from all 100 random forest objects
RF_vardata_u <- data.frame(RF_varimp_u$var, RF_varimp_u$MeanDecreaseAccuracy)
#calculate the average MDA estimate for each of the variables
mda_means_u <- aggregate(.~RF_varimp_u.var, data = RF_vardata_u, FUN = mean)
#Order the variable names by their average MDA estimate in decreasing order
mda_means_u$RF_varimp_u.var <- factor(mda_means_u$RF_varimp_u.var, levels= mda_means_u$RF_varimp_u.var[order(mda_means_u$RF_varimp_u.MeanDecreaseAccuracy, decreasing = T)])
#Order the average MDA estimates in decreasing order
mda_means_u <- mda_means_u[order(mda_means_u$RF_varimp_u.MeanDecreaseAccuracy, decreasing = T),]

#Now we can visualize MDA for each variable
RF_Bpd_MDA_100_u <-
  ggplot(mda_means_u, aes(x=RF_varimp_u.var, y = RF_varimp_u.MeanDecreaseAccuracy)) +
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) +
  theme(axis.text.x = element_text(angle=60, hjust=1,size=10)) +
  theme(axis.text.y = element_text(size=10)) +
  ggtitle("Bd status Random Forest - Upsampling - 100 iterations") +
  theme(axis.title = element_text(size=12,face="bold")) +
  xlab("Variable") + ylab("Importance (Mean Decrease Accuracy)")
RF_Bpd_MDA_100_u

## Also visualize MDA for each class, uninfected and infected, separately.
#isolate all MDA estimates from all 100 random forest objects for infected and uninfected.
RF_vardata_Bd0_u <- data.frame(RF_varimp_u$var, RF_varimp_u$X0)
RF_vardata_Bd1_u <- data.frame(RF_varimp_u$var, RF_varimp_u$X1)
#calculate the average MDA estimate for each of the 46 variables
mda_means_Bd0_u <- aggregate(.~RF_varimp_u.var, data = RF_vardata_Bd0_u, FUN = mean)
mda_means_Bd1_u <- aggregate(.~RF_varimp_u.var, data = RF_vardata_Bd1_u, FUN = mean)
#Order the variable names by their average MDA estimate in decreasing order
mda_means_Bd0_u$RF_varimp_u.var <- factor(mda_means_Bd0_u$RF_varimp_u.var, levels= mda_means_Bd0_u$RF_varimp_u.var[order(mda_means_Bd0_u$RF_varimp_u.X0, decreasing = T)])
mda_means_Bd1_u$RF_varimp_u.var <- factor(mda_means_Bd1_u$RF_varimp_u.var, levels= mda_means_Bd1_u$RF_varimp_u.var[order(mda_means_Bd1_u$RF_varimp_u.X1, decreasing = T)])
#In decreasing order, sort the values of the average Mean Decrease Gini estimates
mda_means_Bd0_u <- mda_means_Bd0_u[order(mda_means_Bd0_u$RF_varimp_u.X0, decreasing = T),]
mda_means_Bd1_u <- mda_means_Bd1_u[order(mda_means_Bd1_u$RF_varimp_u.X1, decreasing = T),]
#Now we can visualize MDA for each variable
par(mfrow=c(1,2))
ggplot(mda_means_Bd0_u, aes(x=RF_varimp_u.var, y = RF_varimp_u.X0)) +
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) +
  theme(axis.text.x = element_text(angle=60, hjust=1))
ggplot(mda_means_Bd1_u, aes(x=RF_varimp_u.var, y = RF_varimp_u.X1)) +
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) +
  theme(axis.text.x = element_text(angle=60, hjust=1))
```

```{r}
# Mean decrease in gini and save results of MDA and gini.
#isolate all Mean Decrease Gini estimates from all 10 random forest objects
RF_vardata_u <- data.frame(RF_varimp_u$MeanDecreaseGini, RF_varimp_u$var, row.names = NULL)
#calculate the average Mean Decrease Gini estimate for each of the variables
gini_means_u <- aggregate(.~RF_varimp_u.var, data = RF_vardata_u, FUN = mean, row.names = NULL)
#In decreasing order, sort the variables by their average Mean Decrease Gini estimate
gini_means_u$RF_varimp_u.var <- factor(gini_means_u$RF_varimp_u.var, levels= gini_means_u$RF_varimp_u.var[order(gini_means_u$RF_varimp_u.MeanDecreaseGini, decreasing = T)])
#In decreasing order, sort the values of the average Mean Decrease Gini estimates
gini_means_u <- gini_means_u[order(gini_means_u$RF_varimp_u.MeanDecreaseGini, decreasing = T),]
#Plot mean decrease in GINI
ggplot(gini_means_u, aes(x=RF_varimp_u.var, y = RF_varimp_u.MeanDecreaseGini)) +
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) +
  theme(axis.text.x = element_text(angle=60, hjust=1))
RF_Bpd_GINI_100_u <-
  ggplot(gini_means_u, aes(x=RF_varimp_u.var, y = RF_varimp_u.MeanDecreaseGini)) +
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) +
  theme(axis.text.x = element_text(angle=60, hjust=1,size=10)) +
  theme(axis.text.y = element_text(size=10)) +
  ggtitle("Bd status Random Forest - Upsampling - 100 iterations") +
  theme(axis.title = element_text(size=12,face="bold")) +
  xlab("Variable") + ylab("Importance (Mean Decrease Gini)")
RF_Bpd_GINI_100_u
RF_Bpd_GINI_100
#write.csv(gini_means, file="RF_model_upsampling_importance_gini_nolongitude.csv", row.names = T)
#write.csv(mda_means, file="RF_model_upsampling_importance_mda_nolongitude.csv", row.names = T)
RF_model_u
#------------------------FINAL MODEL WITH 100 ITERATIONS (AVERAGED): --------------------------------
# Call:
#  randomForest(formula = as.factor(Status_Bd) ~ ., data = pred_samp_u,      ntree = 2000, importance = TRUE, replace = TRUE, , strata = as.factor(pred_samp_u[,          "Status_Bd"]), sampsize = c(500, 500)) 
#                Type of random forest: classification
#                      Number of trees: 2000
# No. of variables tried at each split: 3
# 
#         OOB estimate of  error rate: 10.58%
# Confusion matrix:
#     0   1 class.error
# 0 923 142  0.13333333
# 1  27 505  0.05075188
```

#### Downsampling

```{r}
#declare data frames to hold error rates and variable importance measures
set.seed(123)
RF_model_error = data.frame()
RF_varimp = data.frame()

#declare training datasets for both classes
Bd0=RF_Bdp.1[RF_Bdp.1$Status_Bd=="0",] #1065
Bd1=RF_Bdp.1[RF_Bdp.1$Status_Bd=="1",] #216
  
# #determine downsample.size by the minimum number of observations between the two class datasets
downsample.size <- min(c(nrow(Bd0), nrow(Bd1)))
downsample.size #216
                   
library(caret)
for (i in 1:100) {
  
    # #sample from the larger dataset at the downsample size, or size of the smaller dataset at random
   Bd0samp <- Bd0[(sample(nrow(Bd0), size=downsample.size)),] #DOWNSAMPLE SIZE

  # #create training dataset to be used in random forest function (randomized for each iteration)
   pred_samp=rbind(Bd1, Bd0samp)

  #construct random forest object
  RF_model <- randomForest(as.factor(Status_Bd)~.
                  , data=pred_samp, ntree=2000, importance=T
                  , strata = as.factor(pred_samp[,"Status_Bd"])
                  , sampsize = c(216, 216) #subsample equally, so to build a model without majority class bias
                  , mtry = best.m1[1] #choosing 3 for best mtry 
                  )

  #extract the final estimate of OOB error rates
  err=RF_model$err.rate[nrow(RF_model$err.rate),]
  
  #append OOB error estimate to dataframe
  RF_model_error<-rbind(RF_model_error, err)
  colnames(RF_model_error) <- names(err)
  
  #extract the importance measures 
  imp = importance(RF_model,scale=TRUE)
  
  #append importance measures to dataframe
  imp_temp <- data.frame(imp)
  imp_temp$var <- row.names(imp_temp)
  RF_varimp <- rbind(RF_varimp, imp_temp)
}
down_Bd_CM <- confusionMatrix(predict(RF_model, pred_samp), pred_samp$Status_Bd, positive = '1')
down_Bd_CM
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction   0   1
#          0 213   1
#          1   3 215
#                                           
#                Accuracy : 0.9907          
#                  95% CI : (0.9765, 0.9975)
#     No Information Rate : 0.5             
#     P-Value [Acc > NIR] : <2e-16          
#                                           
#                   Kappa : 0.9815          
#                                           
#  Mcnemar's Test P-Value : 0.6171          
#                                           
#             Sensitivity : 0.9954          
#             Specificity : 0.9861          
#          Pos Pred Value : 0.9862          
#          Neg Pred Value : 0.9953          
#              Prevalence : 0.5000          
#          Detection Rate : 0.4977          
#    Detection Prevalence : 0.5046          
#       Balanced Accuracy : 0.9907          
#                                           
#        'Positive' Class : 1      

fourfoldplot(as.table(down_Bd_CM),color=c("red","#40B0A6"),main = "Confusion Matrix" )
```

Downsampling, validation, and assessment

```{r}
####################################################################
#### EXAMINE CLASSIFICATION ERRORS

## See how many were misclassified.
#get prediction probability for each group
Predicted <- RF_model$votes
#get Family names and infection status from data
SampleInfo <- pred_samp[c(1,3)]
StatusPredicted <- cbind(Predicted,SampleInfo)

#see what this information looks like
head(StatusPredicted)

#output these predictions to a file
write.csv(StatusPredicted, file="RF_FINAL_downsampling_model_misclassified_20240526.csv", row.names=FALSE)

#visualize the predictions for each group.
#put predictions for each category in separate data frame
StatusPredicted_Pos <- StatusPredicted[ which(StatusPredicted$Status_Bd == '1'), 1:2]
StatusPredicted_Neg <- StatusPredicted[ which(StatusPredicted$Status_Bd == '0'), 1:2]
StatusPredicted_Pos

#make plots showing the classification of each Out-of-Bag sample
par(mfrow=c(1,2))
barplot(as.matrix(StatusPredicted_Pos), col ="black", main = "Pos")
barplot(as.matrix(StatusPredicted_Neg), col ="black", main = "Neg")

RF_model_error
colMeans(RF_model_error,na.rm=T)
#The proportion of the Out-of-Bag Error (incorrectly assigned) - each run through the forest, the average wins - if overall the trees are incorrect, then the out-of-bag for that run is incorrect. Therefore this is the rate across all runs as the average of each run collectively.
# 
#       OOB         0         1 
# 0.2361343 0.2359722 0.2362963 
write.csv(RF_model_error,file = "RF_model_downsampling_error_nolongitude_20240526.csv",row.names=F)

```

```{r}
# Now that variable importance data is stored, extract the MDA for all variables from each random forest and summarize.
#isolate all MDA estimates from all 100 random forest objects
RF_vardata <- data.frame(RF_varimp$var, RF_varimp$MeanDecreaseAccuracy)
#calculate the average MDA estimate for each of the variables
mda_means <- aggregate(.~RF_varimp.var, data = RF_vardata, FUN = mean)

#Order the variable names by their average MDA estimate in decreasing order
mda_means$RF_varimp.var <- factor(mda_means$RF_varimp.var, levels= mda_means$RF_varimp.var[order(mda_means$RF_varimp.MeanDecreaseAccuracy, decreasing = T)])

#Order the average MDA estimates in decreasing order
mda_means <- mda_means[order(mda_means$RF_varimp.MeanDecreaseAccuracy, decreasing = T),]

#Now we can visualize MDA for each variable
RF_Bpd_MDA_100 <-
  ggplot(mda_means, aes(x=RF_varimp.var, y = RF_varimp.MeanDecreaseAccuracy)) + 
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) +
  theme(axis.text.x = element_text(angle=60, hjust=1,size=10)) + 
  theme(axis.text.y = element_text(size=10)) + 
  ggtitle("Bd status Random Forest") +
  theme(axis.title = element_text(size=12,face="bold")) + 
  xlab("") + 
  ylab("")
RF_Bpd_MDA_100

## Also visualize MDA for each class, uninfected and infected, separately.

#isolate all MDA estimates from all 100 random forest objects for infected and uninfected.
RF_vardata_Bd0 <- data.frame(RF_varimp$var, RF_varimp$X0)
RF_vardata_Bd1 <- data.frame(RF_varimp$var, RF_varimp$X1)

#calculate the average MDA estimate for each of the 46 variables
mda_means_Bd0 <- aggregate(.~RF_varimp.var, data = RF_vardata_Bd0, FUN = mean)
mda_means_Bd1 <- aggregate(.~RF_varimp.var, data = RF_vardata_Bd1, FUN = mean)

#Order the variable names by their average MDA estimate in decreasing order
mda_means_Bd0$RF_varimp.var <- factor(mda_means_Bd0$RF_varimp.var, levels= mda_means_Bd0$RF_varimp.var[order(mda_means_Bd0$RF_varimp.X0, decreasing = T)])
mda_means_Bd1$RF_varimp.var <- factor(mda_means_Bd1$RF_varimp.var, levels= mda_means_Bd1$RF_varimp.var[order(mda_means_Bd1$RF_varimp.X1, decreasing = T)])

#In decreasing order, sort the values of the average Mean Decrease Gini estimates
mda_means_Bd0 <- mda_means_Bd0[order(mda_means_Bd0$RF_varimp.X0, decreasing = T),]
mda_means_Bd1 <- mda_means_Bd1[order(mda_means_Bd1$RF_varimp.X1, decreasing = T),]

#Now we can visualize MDA for each variable
par(mfrow=c(1,2))
Uninfected_MDA <- ggplot(
  mda_means_Bd0, aes(x=RF_varimp.var, y = RF_varimp.X0)) + 
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) +
  theme(axis.text.x = element_text(angle=60, hjust=1)) + 
  ggtitle("Bd status Random Forest - Uninfected") +
  theme(axis.title = element_text(size=12,face="bold")) + 
  xlab("") + 
  ylab("")

Infected_MDA <-ggplot(
  mda_means_Bd1, aes(x=RF_varimp.var, y = RF_varimp.X1)) + 
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) +
  theme(axis.text.x = element_text(angle=60, hjust=1)) +
  ggtitle("Bd status Random Forest - Infected") +
  theme(axis.title = element_text(size=12,face="bold")) + 
  xlab("") + 
  ylab("")
```

```{r}
# Mean decrease in gini and save results of MDA and gini.
#isolate all Mean Decrease Gini estimates from all 10 random forest objects
RF_vardata <- data.frame(RF_varimp$MeanDecreaseGini, RF_varimp$var, row.names = NULL)

#calculate the average Mean Decrease Gini estimate for each of the variables
gini_means <- aggregate(.~RF_varimp.var, data = RF_vardata, FUN = mean, row.names = NULL)

#In decreasing order, sort the variables by their average Mean Decrease Gini estimate
gini_means$RF_varimp.var <- factor(gini_means$RF_varimp.var, levels= gini_means$RF_varimp.var[order(gini_means$RF_varimp.MeanDecreaseGini, decreasing = T)])

#In decreasing order, sort the values of the average Mean Decrease Gini estimates
gini_means <- gini_means[order(gini_means$RF_varimp.MeanDecreaseGini, decreasing = T),]

#Plot mean decrease in GINI 
ggplot(gini_means, aes(x=RF_varimp.var, y = RF_varimp.MeanDecreaseGini)) + 
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) +
  theme(axis.text.x = element_text(angle=60, hjust=1))

RF_Bpd_GINI_100 <-
  ggplot(gini_means, aes(x=RF_varimp.var, y = RF_varimp.MeanDecreaseGini)) + 
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) +
  theme(axis.text.x = element_text(angle=60, hjust=1,size=10)) + 
  theme(axis.text.y = element_text(size=10)) + 
  ggtitle("Bd status Random Forest") +
  theme(axis.title = element_text(size=12,face="bold")) + 
  xlab("Variable") + ylab("Importance (Mean Decrease Gini)")
RF_Bpd_GINI_100

write.csv(gini_means, file="BdStatus_RF_model_Downsampling_importance_gini_nolongitude.csv_20240526", row.names = T)
write.csv(mda_means, file="BdStatus_RF_model_Downsampling_importance_mda_nolongitude_20240526.csv", row.names = T)
RF_model
#------------------------FINAL MODEL WITH 100 ITERATIONS (AVERAGED): --------------------------------
# Call:
#  randomForest(formula = as.factor(Status_Bd) ~ ., data = pred_samp,      ntree = 2000, importance = T, strata = as.factor(pred_samp[,          "Status_Bd"]), sampsize = c(216, 216), mtry = best.m1[1]) 
#                Type of random forest: classification
#                      Number of trees: 2000
# No. of variables tried at each split: 3
# 
#         OOB estimate of  error rate: 24.07%
# Confusion matrix:
#     0   1 class.error
# 0 166  50   0.2314815
# 1  54 162   0.2500000
```
#------------------------------------------------------
#STEP 8: Validation of final model
We chose model 1 (-longitude) and to balance via downsampling for this model.
```{r}
set.seed(123)
RF_model_error = data.frame()
RF_varimp = data.frame()
Conf_Matrix = data.frame()

split_ds_Bdp <- sample(c(TRUE, FALSE), nrow(RF_Bdp.1), replace = TRUE, prob = c(0.7, 0.3))
train_ds_Bdp <- RF_Bdp.1[split_ds_Bdp, ] #735 negative, 144 positive (Bagging data)
test_ds_Bdp <- RF_Bdp.1[!split_ds_Bdp, ] #330 negative, 72 positive (Out-of-Bag data)
dim(train_ds_Bdp)
dim(test_ds_Bdp)

#declare training datasets for both classes
Bd0=train_ds_Bdp[train_ds_Bdp$Status_Bd=="0",] #735
Bd1=train_ds_Bdp[train_ds_Bdp$Status_Bd=="1",] #144 

Bdpt <- as.factor(train_ds_Bdp$Status_Bd)
mtry2_Bdp <- tuneRF(train_ds_Bdp[-1], Bdpt, ntreeTry = 500, stepFactor = 1.5, improve = 0.01, trace = T, plot = T)
best.m2 <- mtry2_Bdp[mtry2_Bdp[, 2] == min(mtry2_Bdp[, 2]), 1]
print(mtry2_Bdp)
print(best.m2) #2

#ONE WAY TO DOWNSAMPLE THE DATA 50:50 BASED ON NUMBER OF MINORITY CASES
# Sample_data <- train_ds_Bdp[sort(c(sample(which(train_ds_Bdp$Status_Bd == 0), sum(train_ds_Bdp$Status_Bd == 1)),
#                          which(train_ds_Bdp$Status_Bd == 1))), ]
# dim(Sample_data) #149:149

# #determine upsample or downsample.size by the minimum or maximum number of observations between the two class datasets
# upsample.size <- max(c(nrow(Bd0), nrow(Bd1)))
# upsample.size #756 (this duplicates the minority cases until they also meet the majority number of cases)

downsample.size <- min(c(nrow(Bd1), nrow(Bd0)))
downsample.size #144

library(caret)
for (i in 1:100) {
  
    # #sample from the larger dataset at the downsample size, or size of the smaller dataset at random
   Bd0samp <- Bd0[(sample(nrow(Bd0), size=downsample.size)),] #DOWNSAMPLE SIZE

  # #create training dataset to be used in random forest function (randomized for each iteration)
   pred_samp=rbind(Bd1, Bd0samp)

  #construct random forest object
  RF_model <- randomForest(as.factor(Status_Bd)~.
                  , data=pred_samp, ntree=2000, importance=T
                  , strata = as.factor(pred_samp[,"Status_Bd"])
                  , sampsize = c(140, 140) #subsample at similar rates to what we find in nature
                  , mtry = best.m1[1] #choosing best mtry 
                  )

  
  #extract the final estimate of OOB error rates
  err=RF_model$err.rate[nrow(RF_model$err.rate),]
  
  #append OOB error estimate to dataframe
  RF_model_error<-rbind(RF_model_error, err)
  colnames(RF_model_error) <- names(err)
  
  #extract the importance measures 
  imp = importance(RF_model,scale=TRUE)
  
  #append importance measures to dataframe
  imp_temp <- data.frame(imp)
  imp_temp$var <- row.names(imp_temp)
  RF_varimp <- rbind(RF_varimp, imp_temp)
}

varImpPlot(RF_model)
m_bdp <- vip(RF_model, geom = "point",
          mapping = NULL,
          aesthetics = list(),
          horizontal = TRUE,
          include_type = TRUE)
m_bdp <- m_bdp + geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) + ggtitle("Random Forest classification - Trained Model Set for Bd infection status")
m_bdp

## Look at variable importance:
round(importance(RF_model), 2)
#               0     1 MeanDecreaseAccuracy MeanDecreaseGini
# Sp        61.82 29.90                77.85            28.81
# Fam       33.67 35.95                39.77            13.87
# Sex        3.49 11.06                 8.33             6.48
# Age        7.73 13.02                13.37             5.13
# Latitude  35.01 29.23                45.00            26.66
# Elevation 42.30 14.59                47.68            21.68
# TPC1      40.53 32.55                48.64            25.31
# TPC2      17.81 41.91                37.64            22.63
# PPC1      37.71 24.59                46.98            23.27
# PPC2      33.65 34.00                46.38            23.76

#Prediction & Confusion Matrix – train data


ConfMatrix
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction   0   1
#          0 246  19
#          1  84  53
#                                           
#                Accuracy : 0.7438          
#                  95% CI : (0.6982, 0.7858)
#     No Information Rate : 0.8209          
#     P-Value [Acc > NIR] : 1               
#                                           
#                   Kappa : 0.3559          
#                                           
#  Mcnemar's Test P-Value : 2.861e-10       
#                                           
#             Sensitivity : 0.7361          
#             Specificity : 0.7455          
#          Pos Pred Value : 0.3869          
#          Neg Pred Value : 0.9283          
#              Prevalence : 0.1791          
#          Detection Rate : 0.1318          
#    Detection Prevalence : 0.3408          
#       Balanced Accuracy : 0.7408          
#                                           
#        'Positive' Class : 1 

fourfoldplot(as.table(ConfMatrix),color=c("red","#40B0A6"),main = "Confusion Matrix" )

```

#Tissue type - model check
Checking that our model output matches after removing internal-only tissues.

```{r}
#declare data frames to hold error rates and variable importance measures
set.seed(123)
RF_model_error = data.frame()
RF_varimp = data.frame()

#declare training datasets for both classes
Bd0=RF_Bdp.1.NoSE[RF_Bdp.1.NoSE$Status_Bd=="0",] #379
Bd1=RF_Bdp.1.NoSE[RF_Bdp.1.NoSE$Status_Bd=="1",] #142
  
# #determine downsample.size by the minimum number of observations between the two class datasets
downsample.size <- min(c(nrow(Bd0), nrow(Bd1)))
downsample.size #142
                   
library(caret)
for (i in 1:100) {
  
    # #sample from the larger dataset at the downsample size, or size of the smaller dataset at random
   Bd0samp <- Bd0[(sample(nrow(Bd0), size=downsample.size)),] #DOWNSAMPLE SIZE

  # #create training dataset to be used in random forest function (randomized for each iteration)
   pred_samp=rbind(Bd1, Bd0samp)

  #construct random forest object
  RF_model <- randomForest(as.factor(Status_Bd)~.
                  , data=pred_samp, ntree=2000, importance=T
                  , strata = as.factor(pred_samp[,"Status_Bd"])
                  , sampsize = c(142, 142) #subsample equally, so to build a model without majority class bias
                  , mtry = best.m4[1] #choosing 3 for best mtry 
                  )

  #extract the final estimate of OOB error rates
  err=RF_model$err.rate[nrow(RF_model$err.rate),]
  
  #append OOB error estimate to dataframe
  RF_model_error<-rbind(RF_model_error, err)
  colnames(RF_model_error) <- names(err)
  
  #extract the importance measures 
  imp = importance(RF_model,scale=TRUE)
  
  #append importance measures to dataframe
  imp_temp <- data.frame(imp)
  imp_temp$var <- row.names(imp_temp)
  RF_varimp <- rbind(RF_varimp, imp_temp)
}
down_Bd_CM_exto <- confusionMatrix(predict(RF_model, pred_samp), pred_samp$Status_Bd, positive = '1')
down_Bd_CM_exto 
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction   0   1
#          0 139   7
#          1   3 135
#                                          
#                Accuracy : 0.9648         
#                  95% CI : (0.9362, 0.983)
#     No Information Rate : 0.5            
#     P-Value [Acc > NIR] : <2e-16         
#                                          
#                   Kappa : 0.9296         
#                                          
#  Mcnemar's Test P-Value : 0.3428         
#                                          
#             Sensitivity : 0.9507         
#             Specificity : 0.9789         
#          Pos Pred Value : 0.9783         
#          Neg Pred Value : 0.9521         
#              Prevalence : 0.5000         
#          Detection Rate : 0.4754         
#    Detection Prevalence : 0.4859         
#       Balanced Accuracy : 0.9648         
#                                          
#        'Positive' Class : 1       

fourfoldplot(as.table(down_Bd_CM_NoSE),color=c("red","#40B0A6"),main = "Confusion Matrix" )
```

Downsampling, validation, and assessment

```{r}
####################################################################
#### EXAMINE CLASSIFICATION ERRORS

## See how many were misclassified.
#get prediction probability for each group
Predicted <- RF_model$votes
#get Family names and infection status from data
SampleInfo <- pred_samp[c(1,3)]
StatusPredicted <- cbind(Predicted,SampleInfo)

#see what this information looks like
head(StatusPredicted)

#output these predictions to a file
#write.csv(StatusPredicted, file="RF_FINAL_downsampling_model_misclassified_20240526.csv", row.names=FALSE)

#visualize the predictions for each group.
#put predictions for each category in separate data frame
StatusPredicted_Pos <- StatusPredicted[ which(StatusPredicted$Status_Bd == '1'), 1:2]
StatusPredicted_Neg <- StatusPredicted[ which(StatusPredicted$Status_Bd == '0'), 1:2]
StatusPredicted_Pos

#make plots showing the classification of each Out-of-Bag sample
par(mfrow=c(1,2))
barplot(as.matrix(StatusPredicted_Pos), col ="black", main = "Pos")
barplot(as.matrix(StatusPredicted_Neg), col ="black", main = "Neg")

RF_model_error
colMeans(RF_model_error,na.rm=T)
#The proportion of the Out-of-Bag Error (incorrectly assigned) - each run through the forest, the average wins - if overall the trees are incorrect, then the out-of-bag for that run is incorrect. Therefore this is the rate across all runs as the average of each run collectively.
# 
#       OOB         0         1 
# 0.3310915 0.3354225 0.3267606 
#write.csv(RF_model_error,file = "RF_model_downsampling_error_nolongitude_20240526.csv",row.names=F)

```

```{r}
# Now that variable importance data is stored, extract the MDA for all variables from each random forest and summarize.
#isolate all MDA estimates from all 100 random forest objects
RF_vardata <- data.frame(RF_varimp$var, RF_varimp$MeanDecreaseAccuracy)
#calculate the average MDA estimate for each of the variables
mda_means <- aggregate(.~RF_varimp.var, data = RF_vardata, FUN = mean)

#Order the variable names by their average MDA estimate in decreasing order
mda_means$RF_varimp.var <- factor(mda_means$RF_varimp.var, levels= mda_means$RF_varimp.var[order(mda_means$RF_varimp.MeanDecreaseAccuracy, decreasing = T)])

#Order the average MDA estimates in decreasing order
mda_means <- mda_means[order(mda_means$RF_varimp.MeanDecreaseAccuracy, decreasing = T),]

#Now we can visualize MDA for each variable
RF_Bpd_MDA_100 <-
  ggplot(mda_means, aes(x=RF_varimp.var, y = RF_varimp.MeanDecreaseAccuracy)) + 
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) +
  theme(axis.text.x = element_text(angle=60, hjust=1,size=10)) + 
  theme(axis.text.y = element_text(size=10)) + 
  ggtitle("Bd status Random Forest") +
  theme(axis.title = element_text(size=12,face="bold")) + 
  xlab("Variable") + ylab("Importance (Mean Decrease Accuracy)")
RF_Bpd_MDA_100

## Also visualize MDA for each class, uninfected and infected, separately.

#isolate all MDA estimates from all 100 random forest objects for infected and uninfected.
RF_vardata_Bd0 <- data.frame(RF_varimp$var, RF_varimp$X0)
RF_vardata_Bd1 <- data.frame(RF_varimp$var, RF_varimp$X1)

#calculate the average MDA estimate for each of the 46 variables
mda_means_Bd0 <- aggregate(.~RF_varimp.var, data = RF_vardata_Bd0, FUN = mean)
mda_means_Bd1 <- aggregate(.~RF_varimp.var, data = RF_vardata_Bd1, FUN = mean)

#Order the variable names by their average MDA estimate in decreasing order
mda_means_Bd0$RF_varimp.var <- factor(mda_means_Bd0$RF_varimp.var, levels= mda_means_Bd0$RF_varimp.var[order(mda_means_Bd0$RF_varimp.X0, decreasing = T)])
mda_means_Bd1$RF_varimp.var <- factor(mda_means_Bd1$RF_varimp.var, levels= mda_means_Bd1$RF_varimp.var[order(mda_means_Bd1$RF_varimp.X1, decreasing = T)])

#In decreasing order, sort the values of the average Mean Decrease Gini estimates
mda_means_Bd0 <- mda_means_Bd0[order(mda_means_Bd0$RF_varimp.X0, decreasing = T),]
mda_means_Bd1 <- mda_means_Bd1[order(mda_means_Bd1$RF_varimp.X1, decreasing = T),]

#Now we can visualize MDA for each variable
par(mfrow=c(1,2))
Uninfected_MDA <- ggplot(
  mda_means_Bd0, aes(x=RF_varimp.var, y = RF_varimp.X0)) + 
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) +
  theme(axis.text.x = element_text(angle=60, hjust=1)) + 
  ggtitle("Bd status Random Forest - Uninfected") +
  theme(axis.title = element_text(size=12,face="bold")) + 
  xlab("Variable") + ylab("Importance (Mean Decrease Accuracy)")
Uninfected_MDA

Infected_MDA <-ggplot(
  mda_means_Bd1, aes(x=RF_varimp.var, y = RF_varimp.X1)) + 
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) +
  theme(axis.text.x = element_text(angle=60, hjust=1)) +
  ggtitle("Bd status Random Forest - Infected") +
  theme(axis.title = element_text(size=12,face="bold")) + 
  xlab("Variable") + ylab("Importance (Mean Decrease Accuracy)")
Infected_MDA
```

```{r}
# Mean decrease in gini and save results of MDA and gini.
#isolate all Mean Decrease Gini estimates from all 10 random forest objects
RF_vardata <- data.frame(RF_varimp$MeanDecreaseGini, RF_varimp$var, row.names = NULL)

#calculate the average Mean Decrease Gini estimate for each of the variables
gini_means <- aggregate(.~RF_varimp.var, data = RF_vardata, FUN = mean, row.names = NULL)

#In decreasing order, sort the variables by their average Mean Decrease Gini estimate
gini_means$RF_varimp.var <- factor(gini_means$RF_varimp.var, levels= gini_means$RF_varimp.var[order(gini_means$RF_varimp.MeanDecreaseGini, decreasing = T)])

#In decreasing order, sort the values of the average Mean Decrease Gini estimates
gini_means <- gini_means[order(gini_means$RF_varimp.MeanDecreaseGini, decreasing = T),]

#Plot mean decrease in GINI 
ggplot(gini_means, aes(x=RF_varimp.var, y = RF_varimp.MeanDecreaseGini)) + 
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) +
  theme(axis.text.x = element_text(angle=60, hjust=1))

RF_Bpd_GINI_100 <-
  ggplot(gini_means, aes(x=RF_varimp.var, y = RF_varimp.MeanDecreaseGini)) + 
  geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) +
  theme(axis.text.x = element_text(angle=60, hjust=1,size=10)) + 
  theme(axis.text.y = element_text(size=10)) + 
  ggtitle("Bd status Random Forest") +
  theme(axis.title = element_text(size=12,face="bold")) + 
  xlab("Variable") + ylab("Importance (Mean Decrease Gini)")
RF_Bpd_GINI_100

write.csv(gini_means, file="BdStatus_RF_model_Downsampling_importance_gini_nolongitude.csv_20240526", row.names = T)
write.csv(mda_means, file="BdStatus_RF_model_Downsampling_importance_mda_nolongitude_20240526.csv", row.names = T)
RF_model
#------------------------FINAL MODEL WITH 100 ITERATIONS (AVERAGED): --------------------------------
# Call:
#  randomForest(formula = as.factor(Status_Bd) ~ ., data = pred_samp,      ntree = 2000, importance = T, strata = as.factor(pred_samp[,          "Status_Bd"]), sampsize = c(216, 216), mtry = best.m1[1]) 
#                Type of random forest: classification
#                      Number of trees: 2000
# No. of variables tried at each split: 3
# 
#         OOB estimate of  error rate: 24.07%
# Confusion matrix:
#     0   1 class.error
# 0 166  50   0.2314815
# 1  54 162   0.2500000
```

```{r}
set.seed(123)
RF_model_error = data.frame()
RF_varimp = data.frame()

split_ds_Bdp <- sample(c(TRUE, FALSE), nrow(RF_Bdp.1), replace = TRUE, prob = c(0.7, 0.3))
train_ds_Bdp <- RF_Bdp.1[split_ds_Bdp, ] #(Bagging data)
test_ds_Bdp <- RF_Bdp.1[!split_ds_Bdp, ] #(Out-of-Bag data)
dim(train_ds_Bdp)
dim(test_ds_Bdp)

#declare training datasets for both classes
Bd0=train_ds_Bdp[train_ds_Bdp$Status_Bd=="0",] 
Bd1=train_ds_Bdp[train_ds_Bdp$Status_Bd=="1",] 

Bdpt <- as.factor(train_ds_Bdp$Status_Bd)
mtry2_Bdp <- tuneRF(train_ds_Bdp[-1], Bdpt, ntreeTry = 500, stepFactor = 1.5, improve = 0.01, trace = T, plot = T)
best.m2 <- mtry2_Bdp[mtry2_Bdp[, 2] == min(mtry2_Bdp[, 2]), 1]
print(mtry2_Bdp)
print(best.m2) 

# #determine upsample or downsample.size by the minimum or maximum number of observations between the two class datasets
# upsample.size <- max(c(nrow(Bd0), nrow(Bd1)))
# upsample.size #756 (this duplicates the minority cases until they also meet the majority number of cases)

downsample.size <- min(c(nrow(Bd1), nrow(Bd0)))
downsample.size 

library(caret)
for (i in 1:100) {
  
    # #sample from the larger dataset at the downsample size, or size of the smaller dataset at random
   Bd0samp <- Bd0[(sample(nrow(Bd0), size=downsample.size)),] #DOWNSAMPLE SIZE

  # #create training dataset to be used in random forest function (randomized for each iteration)
   pred_samp=rbind(Bd1, Bd0samp)

  #construct random forest object
  RF_model <- randomForest(as.factor(Status_Bd)~.
                  , data=pred_samp, ntree=2000, importance=T
                  , strata = as.factor(pred_samp[,"Status_Bd"])
                  , sampsize = c(140, 140) #subsample at similar rates to what we find in nature
                  , mtry = best.m1[1] #choosing best mtry 
                  )

  
  #extract the final estimate of OOB error rates
  err=RF_model$err.rate[nrow(RF_model$err.rate),]
  
  #append OOB error estimate to dataframe
  RF_model_error<-rbind(RF_model_error, err)
  colnames(RF_model_error) <- names(err)
  
  #extract the importance measures 
  imp = importance(RF_model,scale=TRUE)
  
  #append importance measures to dataframe
  imp_temp <- data.frame(imp)
  imp_temp$var <- row.names(imp_temp)
  RF_varimp <- rbind(RF_varimp, imp_temp)
}

varImpPlot(RF_model)
m_bdp <- vip(RF_model, geom = "point",
          mapping = NULL,
          aesthetics = list(),
          horizontal = TRUE,
          include_type = TRUE)
m_bdp <- m_bdp + geom_point(shape = 21, colour = "black", fill = "#40B0A6", size = 4, stroke = 1) + ggtitle("Random Forest classification - Trained Model Set for Bd infection status")
m_bdp

## Look at variable importance:
round(importance(RF_model), 2)
#               0     1 MeanDecreaseAccuracy MeanDecreaseGini
# Sp        61.82 29.90                77.85            28.81
# Fam       33.67 35.95                39.77            13.87
# Sex        3.49 11.06                 8.33             6.48
# Age        7.73 13.02                13.37             5.13
# Latitude  35.01 29.23                45.00            26.66
# Elevation 42.30 14.59                47.68            21.68
# TPC1      40.53 32.55                48.64            25.31
# TPC2      17.81 41.91                37.64            22.63
# PPC1      37.71 24.59                46.98            23.27
# PPC2      33.65 34.00                46.38            23.76

#Prediction & Confusion Matrix – train data
ConfMat_Val <- confusionMatrix(predict(RF_model, test_ds_Bdp), test_ds_Bdp$Status_Bd, positive = '1')
ConfMat_Val
# Confusion Matrix and Statistics
# 
#           Reference
# Prediction   0   1
#          0 246  19
#          1  84  53
#                                           
#                Accuracy : 0.7438          
#                  95% CI : (0.6982, 0.7858)
#     No Information Rate : 0.8209          
#     P-Value [Acc > NIR] : 1               
#                                           
#                   Kappa : 0.3559          
#                                           
#  Mcnemar's Test P-Value : 2.861e-10       
#                                           
#             Sensitivity : 0.7361          
#             Specificity : 0.7455          
#          Pos Pred Value : 0.3869          
#          Neg Pred Value : 0.9283          
#              Prevalence : 0.1791          
#          Detection Rate : 0.1318          
#    Detection Prevalence : 0.3408          
#       Balanced Accuracy : 0.7408          
#                                           
#        'Positive' Class : 1 

fourfoldplot(as.table(ConfMatrix),color=c("red","#40B0A6"),main = "Confusion Matrix" )
```
#------------------------------------------------------
# Visualization
Figure 3a-c
```{r}
library(patchwork)
(RF_Bpd_MDA_100) /
  (Uninfected_MDA | Infected_MDA)
```
#------------------------------------------------------